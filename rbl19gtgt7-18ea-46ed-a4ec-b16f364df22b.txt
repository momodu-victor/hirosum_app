Get started and then I will try to also share the screen so that you get basically then also to see what we are going to talk about today. Okay, so let's get started today. Now I have again the echo which I didn't want to have and now I hope now I hope things are actually working in that respect, that everything is fine again. Okay, now that I have to move back to my presentation to actually get started. Okay, so let's go for another session. That's the wrong out overview what are we aiming at today? Well, today we are actually looking, so to say, our data science process at the preparatory steps before we actually can really go into an analysis. The exploratory data analysis is already part of that and these steps quite often are not. So to say that sequential as one would like to have them, you have to iteratively go back and forth and combine certain things. So today we will be talking more about actually getting a data set prepared so that one can work with that, clean the data, look at missing values, but also join tables, different tables that belong content wise to the same situation and should be brought together. So that's our focus today. So we will very much talk about relational data, which typically means we have data stored in multiple tables and pairwise. So always two tables somehow have a relation and can be brought together and basically not only in R, but in other programming languages and in computer science in general, we can pretty much focus on three different types of verbs as it is often called that we need to work with such relational data. The one would be taking joins. So really that's the situation where we would like to add new variables from one data frame to another by matching certain observations. So we have, if you take the COVID-19 example, we might have any data sets to say are based on the nation states. So they count all the COVID-19 information, the incidences, the cases for a specific country and there are plenty of other data sets available, tables which has and comprise some information on the country level. So all kind of economic data that is available, all kind of political data that might be available, which maybe at some point is relevant or interesting to see. So what we would like to have is a larger data set where we basically add and combine variables from different data tables, matching so to say, on the case level, on the country level basically. So we have to have some indicator that allows us to match and then we can combine different variables, different features that are there. So that's you're taking joints that we have which are rather widely used, filtering joints, so to say, are operating not on the variable level, but they are operating on the observation level. That means we filter certain observations that metric will not match an observation in the other table, so that I only get the observations that are really included in both tables. So if I have to say different kind of informations, again, I could take the country situation. I might have a data set, that is to say using all countries worldwide. And I have another data set where only a subset of the countries is included, whether it's a regional area or whether it's the OECD countries or whatnot. So there might be a reason that I have to say a data set with a subset of the observations. And then I would like to, if I join only to go to those cases, to those countries that are actually then available in both data tables. And the last set of operations, the set operations, they really are more on a set theoretical level and try and treat all the observations as if there were elements of set. And we look at these things that's the least frequent used operation with relational data that we do, at least in statistics, but it's also available and can be used. Let's try to illustrate that with an example. And so far we always have used the New York City flights data. So let's continue with that. Actually, if you look into that package that is available there in R, there are five data tables, five data frames included. One about the airlines that's the airline names, basically. One about the airports having airport metadata data, the flights data set. So that's the one that so far we have been focusing on in all our examples. One about the planes that are to say used, and one about the weather at the origin airport that is there. So if we look a little bit closer into these data sets. So the airlines data sets is basically just a tibble with 16 rows and two columns. So there are 16 airlines operating and included in the data set. And we have the airline code and the explicit airline name that is available there. For the airports, we have similar things. We also have the FAA airport code, we have the name of the airport, latitude, longitude, altitude of that airport, and a couple of other things that are available there. So there we have a total of eight columns that are there. The planes data set has to say the tail number. That's something we know from the flights data set already, that's what to say. They are typically to each flight there was a tail number assigned, the tail number of the plane, so to know which plane actually operated the flight, and a couple of other things like how old the plane is, what kind of type it is, the manufacturer, and a few more things as well. And then we have the weather data for the origin airport. So for the three departing airports on a given day, on an hourly basis, apparently with temperature, humidity and other things that are given there. So, quite a rich, so to say, collection of information that is available that we quite often would like to say to at least analyze partly together, not necessarily bringing everything into one data frame, but at least to combine. For one of the other things, it might be helpful to have two tables at least combined, or information from two tables at least combined. We can visualize the relations here. If you really go into detail how things belong to each other and refer to each other, there are some relations. Well, the richest data table that we have is the flights data table. That's the reason why we also have been focusing on that so far. And there are some relationships to say that are rather straightforward like the planes, they have tail numbers and tail numbers also variable in the flights. So there's all these airlines, the carrier names and carriers that are straightforward that is there. There are other things that are a little bit less straightforward like the airport, the airport code is basically split then it can be the origin of the flights. So there is a relationship between the airport code and the FAA code and the origin. But there's also a relationship between the FAA and the destination. And the weather data is really linked to the flight data by pretty much five variables altogether, its place and time, so to say that matches the weather data and the flight data. So that is clear here how these things go together. Well, it's nice to have such a drawing and being able to then sort out things, but that's not always a priority feasible in particular, in many instances you don't even know when you get your data tables, you don't even know what are actually the linkages that exist there. So you might just get from a customer who wants to have a data analysis, you might get a huge extract from a database where a lot of things are combined and stored in multiple different tables and you can in principle extract them. And there was some SQL query that they produced these tables but in order to analyze them you really have to find out what are the major ingredients and how can you combine that. So the key issue or the key point there are actually the keys. So what are keys in the context, in the context of relational data? Well, keys are variables that you can use to connect a pair of tables. They are variables. So as in the previous example, so to say the tail number here is potentially a key that allows us to connect the planes table with the flights table, with the weather data. It's actually a combination of various variables that together form the key that is there. So the key can be a single variable or it can be a set of variables. In the end, what they should do is they should uniquely identify what you call an observation or what we call a case in statistics. So that we really say okay, that's our unit of analysis that we are focusing on. So if we are talking about a flight, then it's really a particular flight that is our unique case, unique observation that we would like to have identified by other things. If it's so to say, the weather situation is the weather situation at a given time point, at a given location. So that's the reason why the uniqueness is described on a different level. So in simple cases, a single variable is sufficient. I've mentioned the tail number. Sometimes we need multiple variables like for the metadata, there are typically two types of keys, a primary key, that's the key that uniquely identifies an observation in its own table. So in the planes data set, the table number is the primary key because the data in that table is about planes. So each information that is given there corresponds uniquely to a plane and that's where all other information in that data table refers to it's always information that refers to a plane, the tail number, the manufacturer, how old the plane is, what kind of engine it has and things like that. A foreign key is now an identifier or a key variable that uniquely identifies an observation in another table. So in the flight data frame, Knam is a foreign key because in the flights table the flight is actually the primary but nevertheless the flight table number uniquely matches each flight to a leak plane. So you have and that is uniqueness, a unique identification and it's then called a foreign key and the two types of keys we are going to use and make use of that. So one of the first things if we do not really know which variables are keys or could be keys, if so to say the information is not completely given is that we actually might check for uniqueness of certain observations. So here I just count how often tail numbers really occur and I wanted to filter out those tail numbers that occur multiple times in the planes data set as the planes data set to say identifies each plane, each tail number actually should only occur once. And true, if I say look for those that occur more often than once, I get an empty data set. So there is no pay number occurring twice in the planes data set. In the flights data set there will be multiple times the same ten number because the same plane has multiple flights through here. But in the plane state each ten number only occurs once because it uniquely identifies the planes. We can also look in the weather data whether actually we have uniqueness to say here. Also in terms of that we add every hour, half for each airport, exactly one measurement which apparently is not the case. So that's something where apparently for November 3, 2013 we have two weather, weather information, so to say, for each of the three airports at 01:00. Now, the next question would be are these identical or are they different? If they are identical, it just might be that for whatever reason, these entries have been duplicated. If they are different, then the question is which ones are the correct ones, which ones are the wrong ones, which ones should be included? We would actually expect also here, that we only would have no entry here that is a frequency higher than one. And these three results, they are to say, surprising. If they are correct, then the combination cannot actually be used as as key, a, a primary key. But in this situation we would actually prefer to have that. So we would have to sort out what the reason is and we have to clean the data here and say okay, we presumably have to get rid of the duplicates that are there and check first whether they are duplicate or not. So checking for uniqueness is on the one hand a mandatory part for actually really having a key identifier using a primary key. On the other hand, it also can help to find out duplicates that shouldn't be there and or that at least their existence needs a special interpretation or a special justification that is there. Tables need not have an explicit key. So in the flights data set and that's sort of something that comes a little bit as a surprise also to me, I have to say, in the flight data set we would expect that the combination of year, month, day, flight, so to say, is unique. All tables have an implicit key, simply by the row number, as we are so to say, that's the standard assumption that we all always do. In the tidy data set, the data is organized in such a way that each row corresponds to a different observation, to a different case. So if we talk about the flights and we were always assuming, and I was always assuming that each entry, each row to say, reversed a different flight, that is however not straightforwardly, explicitly replicated in the variables. So whether you take the flights or whether you take tail number in addition to the date, things are not unique. So the tail number, well, that's quite obvious. Planes could operate multiple flights at a date, so that might not be unique, that's quite obvious. But then also the flight variable that is included there is not unique. So to say, you assign apparently some codes are assigned multiple times or there are a lot of recording errors that are in there. That's something I can't touch, so to say straightforwardly at the moment, because I don't know exactly the background or the quality of the data, but these are things that can happen. You need not have an explicit key as implicitly. The row number can always operate implicitly. The row number is used you could and sometimes one has to do that. You can then sometimes just add the row number as an additional variable, really including it into the data frame so that you have a unique explicit key and a unique explicit identifier that you can in particular use if you subset your data to multiple points. And then you want to say, keep control, that you really have these things under control and properly done and not running into wrong matches or wrong joints that are done there. So the primary key plus the corresponding foreign key is they establish the relation. Typically we have one to many relations, so each plane has many flights in our example. So it's a one to N often also abbreviated relationship that we have. So it's a one to n and that's what you say we do. Some relationships are one to one, which is just a special case of one to many. So each flight has one plane, each plane has as many flights. So if you go the opposite direction, each plane has many flights, but each flight has just one plane. Hopefully some relations are many too many. Like you have many airlines and many airports. So there between airlines and airports you have a many to many relationship which you typically can break down or model with many to one relation plus a one to many relation. So each airline flies to many airports and each airport hosts many airlines. So you get things so to say, you get things broken down to two simpler structures then, but you have to keep both up to date and there okay, so these are the different relations and the different operations that we can basically have or the current setting that is there. Now let's look at these three families of works that I have mentioned at the beginning already, the three different operations that we are going to use to actually combine different tables. So we're taking joints, they allow us to combine variables from two tables and they have first to be matched via the keys. So using the primary key and the foreign key to actually match observations and then we can add variables from one table to the other, copy basically variables from one table to the other. Now, just looking at, in our case at a very simple example and in order to be able to show it, so to say, on one screen, I have just taken a selection of a few variables of the flight data set. So I select just a few variables so that I can show on one screen if I add another one. So that's just a subset, basically the six variants of my life data set, only year, Monday, our tele number and carrier. And let's now say, okay, I would like to have the full airline name there and not just the abbreviation, not just the airline code. So I need to join this table with the airlines table and I want to use carrier as a key. And there are, and I will come to that, there are four different joint operations that we have left join, right join, inner join, join and I will explain the differences here. It wouldn't matter here, any of the joins would actually produce the same result as we have basically an identical match in these things and we keep all the things. So the joint operation by carrier then just add to say the information that is additionally available in the airlines data set and that's the name of the full name of the airline and each time, so to say the carrier code is listed there, you get the same airline full name listed here. So you have now a new table where you basically have to say replicated the names multiple times. That's the reason why originally presumably that was separated to save storage stays as you have to say long names which occur. You only have how many? 16 airlines, I guess we only have the airlines data frame was much smaller, much more condensed and now you're you basically have to replicate rather redundantly this information which uses more space and that's typically the reason why things are split out. One of the reasons why things are split over multiple tables because you can actually store them also more efficiently. But that's how things would work. We have a joint operator and we specify the key and as carrier is the same variable name in both instances, that's straightforward. Otherwise we would have to specify what is to say the name in the one, in the one data frame and what is the name and the other and that they should match like with FAA code of the airport and origin and destination. We would have to tell that and match that here as well. So the mutating points, they add columns from one data table to the other data table. So from Y to X and there are basically four different commands we have used the left join, which so to say keeps all the rows that are that's the reason why it's called left join. It keeps the idea is that the join, the join always goes to say from join X and A. So the idea always is that we go from left to right then. So the left choice means we keep all the observations that we have in X and we add then the information from the columns in Y and replicate, so to say if needed, if we have more rows in X then Y, the right join would exactly do the opposite would keep all the rows in Y. So the right join would produce a different result in the previous example than the left join. But inner join and full join, they would actually do the same as to say we have the dominance by X basically. So the inner join includes all the rows in X and y. The full join includes the rows that are in X or in Y. So that really combines both if you have matching situations. So in our case, the left joint is the most appropriate because we want to keep the rows of our flights and we just want to add the information of the airlines. Now the filtering joints, they match the observations in the same way as the mutating joints, but they work, they operate on the observations, not on the variable. And you have two types again, so you either can keep all the observations in X, then also have a matching correspondence in Y, or you can drop them. So you can do the antichoin, you can drop all the observations. And let's also illustrate that with an example. Assume that I would like to find all the flights that go to the top destinations by top destinations, meaning those destinations which have to say the most flights that go to them. So I just count how often a destination for to say occurs in the flight data set and I sort them in descending order and I just take them the top ten. So these are the top ten airport destinations that are in our flight data set. And you see so that's Chicago, the top destination, top airport, Atlanta, it's not surprising. These are the hubs of the big airlines in the US. And it's quite obvious that to say, a lot of flights go there because they distribute flights then from their major hubs. And that's the reason why I suppose that we have quite frequent flights going there. So these are the top destinations and now we can do a semi join to actually say, really get all the flights that go to these top destinations. So basically we have something like 360,000 flights, a little bit less than half of it. So only 140,000 flights go to the top ten destinations. So that's quite a lot that go there and these other lights and all the information that is there. So basically we extract, we create a subset which we also could any joins or quite often these daily joins are actually easier to understand what's going on and also more precisely or easier to formulate in particular, if so to say, you have a combination of different variables that you want to control for and that you want to keep in mind. So the kind of operation that you do is sometimes easier than a straightforward filter which you could do in this situation. In particular, you could do that's an easy, fairly easy situation, but it is sometimes and quite often actually more easy to use these filtering joints than to use a simple filter command. Clearly you could also filter basically by matching to say the destination and saying okay, but then you would have here basically you would have an equality constraint for ten different destinations. So you would have to say okay, the destination has to be the subset of these ten airports and then you could do it, it's doable, but the semijoin definitely looks at this point ways more elegant and also proves in many situations to be more easily understandable and better handable. The anti joint is actually sometimes quite useful to diagnose any mismatches that have happened or that have occurred. For example, if you connect flights and planes, you can see that there are many flights that actually don't have a match in planes, so there are no tail numbers given to the flights. There are actually a variety of reasons and we might come to that later on. But here, just to illustrate, if you use the anti join and then see, so to say, the observations, how they are handling, you see the various planes that we have and we see that there are 2512 flights basically that do not have a tail number. They have a missing value in the ten number. An interesting information here actually the larger part of these flights are presumably canceled flights. So the data set comprises a lot of flights for which all the flight information is missing except all the scheduled information. So the scheduled departure time is there, the scheduled arrival time is there, but no real measurement and sometimes even the tail number is recorded and that is a strong indication that these flights have apparently been cancelled beforehand. And then you don't have to assign a tail number, you don't have to assign a plane if the flight is canceled a couple of hours in advance for whatever reason, whether it's weather conditions, whether it's personnel being on strike or not enough passengers and the flight has been canceled. So there are a variety of reasons why flights get canceled and sometimes if these cancellations early enough, you don't assign tail number. If it's last minute due to weather conditions, then presumably you have already assigned of the claim, but not then. But let's not say the anti join is at least sometimes quite helpful. To find out which of these kind of mismatches do you have? Do you really have to say a lot of missing values in one of the two data sets? If you match two things together that actually should match and the set operations, they are the least frequently used, they are occasionally useful when you really want to break a single complex feature into simpler pieces. If you really have to say a rather complex feature structure, these set operations really work to say theoretically on the set of observations on the complete rows, really compare values of every variable and then you can intersect union or look at the difference. So you can only have the observations that are in both tables or you have only the observations that you have both observation sets for to say, brought together or you have exactly the set theoretical difference, only returning the observations that are in the one and not in the other. So they are similar to full chain and inner join, but they are not identity as they really do not work with keys. But look at all the variables. A few guidelines for joining. What should you keep in mind? Maybe the first thing really is always start by identifying the variables that actually are the primary keys in each table and consider surprises in many directions. I recall a consulting project with a company where I was given a huge amount of data that was extracted from an SQL database. Basically was mapping or was relating to three different steps in a production process and they all belong to the same product in the end, but the production processes, so to say, also had different physical shapes in between. So make it easier. It was about steel production and you have to say three different in most steel production schemes you have three different physical states of your goods that you produce. At the beginning, the steel is fluid. So it's a liquid that you have, and it just flows, and it has a certain and then when it cools down, you have more or less blocks of steel that you work with. And then in the end, you basically form these thin steel plates that you really roll up in the form of a coil. So you have three different physical states but it is the same production process that is behind it. And you can match these things also to a certain degree geometrically to each other. But what I wanted to say is there's a lot of data that you collect and based on the internal procedures that are there, you have for each data set a lot of different identifiers and variables that are there which somehow match, but which in this specific case they have been labeled differently. You could see that apparently the measurements are exactly the same in some degrees, but the labels have been different. So you have had different labels, you have had replicated labels in order so to say that the matching also was in the database feasible and you got a lot of redundant information that needed to be extracted. And at first glance not always. So to say what one expected from the tabular format in the end really was to say the key identifier that was there. So it requires some understanding of the data to really find out which variables are the unique identifiers. At the same time. It also requires to a certain degree that you just look empirically which combination of variables would actually be able to act as unique identifiers in many situations. Well, in particular, if there is not a single key but a set of variables that form the key, you might have numerous combinations of variables that empirically show a chance that they are unique identifier. Not all of them really are good unique identifiers. In terms of that they can be manageable and relate to other tables then. In a more general fashion you might get to say locked up like in the airport data set for example, longitude and latitude will most likely for all the airports exactly specify them. Whether this is something it's presumably nothing that helps you with any of the other tables because the information is not available in any of the other tables. So if you use that as your key, as your primary key, you will not be able to actually make use of these things. Also variables with missing values can simply not be primary keys. You have then observations that apparently have no key and that is not feasible. So you need to have and that's then the other issue. If you have a data set where none of the variables, so to say, is complete, so where every variable has some, at least some missing, you definitely need to introduce to say the row numbers as your unique identifier because otherwise you have problems. You need to somehow make sure that your foreign keys match primary keys in another table only for to establish a relation you need to have a primary key that matches a foreign key. And anti join is quite helpful to actually check that whether this really works and whether this is really available there. And you might sometimes find there on the one hand missing values in the foreign key. As we have seen with the Flights data, we are supposed to say the foreign key tail number has missing values while the primary key does not. That's quite clear. But you also might find data entry errors. If there are supposed to say small typos or just characters exchange, things like that, they will typically show up in these cases or if you have lower case uppercase issues that are sometimes there. Yeah, in large data sets these things just occur and these are quite often situations where you sort of say can find such errors which otherwise otherwise you really have to you really will not find that easily. Maybe if your keys have missing values, your foreign keys in particular, then you need to be thoughtful about inner and full joints as to say some rows, some observations will be dropped, the ones that don't have a match and that might be intended, yes or not. That's something you always have to look at and see whether things work out. Be aware that simply checking the number of rows before and after the join is sometimes not sufficient, that everything worked nicely. While clearly this is the mandatory requirement, it is not always sufficient. So the number of rows before and after they have to say to be to match, depending on what kind of join you do. So if it's a left join, then you know exactly that you need to have as many rows as your left table. If you do a right join, then you need to have as many rows as your right table with the inner join. It's the intersection with the full join. It's such a combination and depending on how many, so to say, duplicates of matching and how many are going in addition. So you have to look in more detail, so to say in particular with an inner join, if there are duplicate keys in both tables, you might actually drop exactly as many rows as there are rows that you add. So that in the end the total number of rows are the same, but they look the same, or they look correct, but they are actually incorrect because you just internally have matched the one with the other and then the errors have canceled basically. And you don't see that first class. You really have to maybe check specific observations where, you know, they should, what do they have been have been joined that you check for certain specific representative individual observations and check whether they are correctly taken over in the join. So a lot of things can go wrong. And joining is really something where you on the one hand will presumably also get a lot of error messages from the program, from the software and saying oh, can't join or you don't get the result that you expect. But it is something that is if you work with data sets that come from multiple sources and that have different tables, you have to work with that. You have to get a handle on these things and learn how to best combine these things. There is no easy way to that, no golden path to succeed that, but just to say a few guidelines that you could consider to avoid the most obvious mistakes and to get things done. Okay, so that's a more detailed look. Now a little bit into the data preprocessing step. So once we have assumed that we now have a data table after the join, after the joining tasks, that we have a data table that has, that is more or less prepared for further analysis. We have already done a little bit of cleaning, but we might do more cleaning. We might have seen in the process that certain things, certain cycles are there, but you might check for more of them. We might do that by data exploration and visualization, see unusual observations. What then quite often is done and is recommended is to say that you separate the variables by types. Quite often you actually do it already a step earlier because already a lot of visualization depends what type of data do you have? Do you have to draw a bar chart or do you have to draw a histogram, for example, as the simplest differentiation between two visualizations that you commonly do in data exploration and which work slightly different for continuous data and categorical data. So typically already a step earlier or in that step, you very often separate continuous from categorized data. Also the question of missing values, then typically the procedures that you use to impute values that you supposedly replace for the missing values they differ for continuous and categorical data and also the third step data transformations that you do they also might be different for or they are different for continuous data and for categorical data. And that means in the end then let's say here you have a couple of steps and in real life they are never as straightforwardly linear as they are. Here quite often you go back and have to clean again and then redo a certain step and you go quite often in circles a little bit that can happen but in principle you try to move on for a few steps, so to say in two paths. One for the continuous data, one for the categorical data to then in the end bring things together in data aggregation and then the two next steps that quite often come are on the one hand dimensionality reduction. And that's something we will start talking about later today and then comes the model training which we will deal in more detail with next week. So now we will talk a little bit about missing values in particular and other data transformations that can be done and other first steps of dimensionality reductions that are done quite often. Okay, missing values occur we have seen in the Flights data set there are numerous missing values and well the first thing is to identify the missing data. I have told you last time when looking at a couple of the histograms that zero values in some of these plots it needed to be clarified whether these zero values are actually missing values or whether they are really measured zeros. And that's supposed to say one of the things with identifying missing data not all of the missing data might have the standard missing data code zeros quite often for numerical values zero value might just be a substitute for the missing value. And you have to find out that it's something you will not find out from the data. You will have to ask the data experts, the ones who have collected the information, who know about the processes, how this data is collected, whether it's not working, what is the default they use to say that a sensor puts into the database if it's not measuring correctly, is it a missing score or is it a zero score? Or is it -999? Or something like that so there can be multiple forms of missing values? Or as they are listed? You have to identify what is actually missing? Then it's always interesting to try to examine what are actually the causes of the missing data for a variety of reasons. For some missing data there might actually be not just a good explanation but actually they mean actually something else. As I said in the flights data set there are canceled flights and canceled flights. They are a category by itself. So it's a very special form of missing this and we presumably could want to analyze certain things of them separately. So knowing that these flights have been canceled, clearly it doesn't make sense to include them for any kind of analysis of arrival delays or something like that. But for many other analysis it would be presumably the major focus of interest to just look at canceled flights. So you could buy that already subdivide the data set and say okay, we can separate certain flights because they did not take place, they just have been planned. The other reason why the cause of the missing data should be examined is well whenever we would like to impute missing values, we actually have to distinguish why they are missing because the different formats that we have, they depend to a certain degree on the statistical treatment, let's put it of missing data. The statistical classification of missing data. In principle you have two big or two options. What you can do with missing values, you either delete missing values or you replace the missing values with reasonable alternative data values, which is called imputation. So you impute the missing values by something that you take from somewhere else. But that depends very much so to say, on the causes of the missingness. From a statistical perspective we differentiate three different reasons or three different categories of missing. This and so to say from a statistical perspective, best situation is if the missings are completely at random and so there is no systematic reason why data is missing. And the consequences, let's say the complete cases, they are a simple random sample from the entire data set. And if in essence deleting the missing completely at random data is in a large data set, then not a big deal as we assume that the missing is completely random, it is some information that gets lost. But that's to say in a large data set we assume still we have sufficient information and just randomly we have a little bit fewer than we originally intended to have. Or the alternative would also be okay. If they are completely missing at random then substituting them by the average score might also not be too bad or might be reasonable. So pretty much you can once data is missing completely at random, you can pretty much also argue with in all directions what to do, always assuming that very little harm is done as to say the missing is really not in a systematic way. The point always is. On the other hand, okay, how many missing do you have and what would be the consequences? Missing at random means that the missingness of the data is not depending on its own unobserved value or its own value. So it's not depending on the variable itself, but it might be depending on other observed variables. And in all other cases we have not missing at random. Now let's put it the arrival, that the arrival delay is missing for canceled flights it's missing at random. It's not depending, so to say, on the own unobserved value. It's basically depending on the fact that the flight did not depart so that we don't have any departure time. And once we don't have any departure time, we also can't have any arrival time. So that is in that context only missing at random. If that's the case, or if we have no arrival time but we have the departure time and we don't have the travel time, the air time, then it's also not missing at random. It's a consequence of the missing of other observed variables. Or what is completely missing at random is, for example, if people in surveys don't want to indicate how much salary they have there, we clearly know that people with a higher salary are more reluctant to indicate that. So this data is really missing due to its own observed value. It's missing due to the fact that people with high income as well as also people with very low income usually are not willing to specify and to answer correctly. So that's the third aspect of missing, that it's not missing at random, that there is really a systematic reason why people do not provide or why misinformation is not available. Now, if you look a little bit just to provide you with an overview of what you can do to identify missing values and then do it and which our packages are available, there are the straightforward questions that are there. There is also the question of for complete cases or not complete cases. In the vim package that provides, to say, some information, you can then typically do three things delete missing values, impute missing values or there is for a few more like this, for a few methods and procedures you can actually do. So to say, with the incomplete data you can run the maximum likelihood estimation and there is a special package that does that. But that's, so to say, for a subset of cases where we actually do that. Typically you either go for deleting missing values or you go for impeaching missing values. And deleting can always be done in two ways. The more frequency is really the casewise or listwise deletion that you really just omit the entire observation, the entire case. So if a flight has one single variable missing, you just delete the entire flight and leave it out and just do the analysis with the complete cases only. That's the most frequent and the most general and the most straightforward way to do the available case pairwise option is only available for some functions. It doesn't make sense for all of them, but for some and some functions do it, so to say, internally they use the available case options like linear regression modeling always tries to use the maximum number of cases that are available. And if you have two models which differ by one variable, by one predictor and the predictor has missing values. The two models also will be based on two different, slightly different data sets because the one they both will use, so to say, the maximum number of available cases and that causes then sometimes problems. So to say, if you go this route, if you compare two different models because actually they are slightly built on two different data sets, the one having more cases than the others, the other because the one model included a variable that has a few missing values by the others the other does not have. So that's another reason why the case wise deletion quite often is preferred as it is to say, more stable in that respect. If you compare different models to each other, you know, if you always run with a complete data set only all the models will be based on exactly the same data, so they will use the same training data. So that's one reason why these things are often preferred. But there is another one that is available imputing missing values. There are basically two different ideas. The one is single imputation where basically we just impute the missing based on the data that is available by either saying, okay, I use the mean value for that variable or I use somehow a random selection of random drawings of the observed values and assign that to that. Or I do multiple imputation where I actually combine different variables. In particular in the missing at random situation that's very prominent. That where I combine those variables that might have an influence on the missing is and then try to use whether it's a linear regression or some other things that are there, where I do multiple imputation method there. And there are a variety of packages that use then typically also Monte Carlo elements at least in the algorithm to impute missing values. We will presumably well, we might in the second half of the semester maybe look at a few more details of multiple imputation or imputation in general. For the time being, we just will use, so to say, the imputed values and might be happy if things work out nicely. Okay, I guess I postpone this to the second part of the lecture after the break and would just open at this point the floor for questions, comments from your side, any questions at this moment? Professor, I have a question. Is there a percentage of tolerance for the mission of value so as not to impact the credibility of outputs? That's a good question and a question that sort of say is well, it is really usually a combination of a variety of things. On the one hand, it's a combination between what is the cause of missing this, is it missing at random, is it missing completely at random, is it not missing at random? And the amount of missing data. That's the one thing. So it's a combination of these two things. And the other thing we should probably also not forget, or we should actually try to keep in mind anyways, the data set that we have is, how shall I say, for many instances we actually assume in our analysis that the data set is somehow a random sample from the entire population that is there. Now, as I said, if the data is missing completely at random, the complete cases are still a representative random sample from the population or from the entire data set that we have. And then if this complete cases data set is still sufficiently large, there is no argument to say why we should not work with a smaller data set. It's just yeah, we have robot some information that might be true, but still we are anyways working under the assumption that we have a sample of representative sample of the population and we can draw inference to the population, to the entire data set that is there. So this inferential aspect is always there. In as long as the remainder of the data set that we analyze still is representative, well, we might lose a little bit, so to say, in terms of inference quality, as the data set gets a bit smaller. But assuming that we work in a world where data is available to a certain abundance, there is little that might cause issues. It also depends a little bit, so to say, on what is the reason for our analysis. If we look for fraud detection, unusual observation, something like that, then actually deleting missing values might be the wrong idea because maybe even so to say, some of the proto and cases are actually characterized by the fact that there are also missing. But that would then be a completely different investigation, a completely different goal that we have to achieve. So that's also a situation where how many missing values we have does not really have a major impact to what we are doing. The most complicated case really is the point where we know that things are missing at random or where they are not random at all and we impute values clearly. If so to say, the amount of imputed data is again, it's difficult to tell you whether there is a reasonable threshold. But intuitively I would say it's clear if for a variable the imputed values are more than the truly originally observed ones, well, the reliability of any analysis that is built somehow on this data is in danger. Whether we rely, so to say, and we allow 25% of a variable to be imputed and 75% should be original, or whether we only allow 20 and 80, or whether we allow 30 and 70 hard to say. I would say that very much depends on the internal structure and I would say there is no right or wrong and there is clearly the fewer the better. But there is also no really good threshold, I have to say. And it depends on too many different aspects in the end. Further questions. Yes. Hi, professor. I had a question about so it was a really nice session and I got to learn a lot. But one thing I kept thinking about was that we are currently also working on data sets related to COVID-19 and generally the data sets are of time series nature, so they are based on dates. So I just wanted to know, is there any best practice or something that we should keep in mind while we are dealing with data sets that have missing value in time series? Like for example, the number of cases might have some missing values, but just imputing them with the mean value would not be a good solution in those cases, just one. With the time series you typically have an additional option because in the time series to say the number of missing is not too long, you have points around before and after and in many instances, so to say interpolating, then the missing values is a very reasonable thing to do. So the time series data has due to its consecutive structure, while it poses quite some additional issues with other things for missing values, it actually offers a little bit more help as to say the lean score between two values if the one in the middle is missing is a very reasonable estimate and choice that is there. So that helps quite a lot. I would say that's the one thing, but nevertheless also there you could use other in particular if you then also have other variables available, you could even use a combination of all these things to actually predict the missing values. So there clearly are, I would say, even some advantages for a time series with missing values as opposed to other data where you do not have a clear time dependent ordering of the data anymore. Okay, I guess we make a break now here and then we can continue also discussions and questions and answers at 215 when we start again. Thank you very much for your attention and see you.